# ‚úàÔ∏è Sky-Safe : Flight & Weather Risk Analytics Pipeline

## üìñ Description du Projet
**Sky-Safe** est une plateforme de traitement de donn√©es Big Data con√ßue pour analyser les risques m√©t√©orologiques li√©s au trafic a√©rien en quasi temps-r√©el. 

La probl√©matique m√©tier que nous r√©solvons est la suivante : *Comment identifier automatiquement les avions commerciaux qui s'appr√™tent √† traverser des zones de turbulences intenses ou d'orages, afin d'optimiser la s√©curit√© des vols ?*

Pour y r√©pondre, notre pipeline ing√®re, nettoie et croise les donn√©es de g√©olocalisation des vols en direct avec les pr√©visions m√©t√©orologiques locales, avant d'exposer un "Score de Risque" sur un tableau de bord interactif.

## üõ†Ô∏è Stack Technique
* **Orchestration :** Apache Airflow (DAG d√©clench√© toutes les 2 minutes)
* **Ingestion (Extract) :** Python (Requests, Pandas)
* **Transformation (Format & Combine) :** Apache Spark (PySpark)
* **Base de donn√©es & Recherche :** Elasticsearch
* **Visualisation :** Kibana
* **Infrastructure :** Docker & Docker Compose
* **Stockage :** Data Lake local organis√© selon l'architecture Medallion

## üì° Sources de Donn√©es (APIs Open Source)
1. **OpenSky Network API** (https://openskynetwork.github.io/opensky-api/) : Suivi des vols en temps r√©el (Latitude, Longitude, Altitude, ICAO24).
2. **Open-Meteo API** (https://open-meteo.com/en/docs) : Donn√©es m√©t√©orologiques haute pr√©cision (Vitesse du vent, Pr√©cipitations, Orages) bas√©es sur un syst√®me de grille g√©ographique.

## üìÇ Architecture du Data Lake (Clean Naming)
Le projet respecte une hi√©rarchie stricte de stockage des fichiers pour garantir la tra√ßabilit√© de la donn√©e : `data/<layer>/<group>/<dataEntity>/date=YYYY-MM-DD/hour=HH/`

* `data/raw/` : Donn√©es JSON brutes fra√Æchement extraites des APIs.
* `data/formatted/` : Donn√©es nettoy√©es, typ√©es et converties au format optimis√© Parquet via Spark.
* `data/enriched/` : Donn√©es massives issues de la jointure spatiale entre les avions et la m√©t√©o (Spark Join).
* `data/usage/` : Donn√©es agr√©g√©es et all√©g√©es, pr√™tes √† √™tre ing√©r√©es par Elasticsearch.

## üöÄ Installation et Lancement

### Pr√©requis
* Docker et Docker Compose install√©s sur votre machine.

### √âtape 1 : D√©marrer l'infrastructure
Placez-vous √† la racine du projet et montez les conteneurs Docker (Airflow, Postgres, Elasticsearch, Kibana) :
`docker-compose up -d --build`

### √âtape 2 : Acc√©der aux interfaces
Une fois les conteneurs d√©marr√©s, les services sont accessibles aux adresses suivantes :
* **Apache Airflow :** http://localhost:8080 (Identifiants : admin / admin)
* **Kibana (Dashboard) :** http://localhost:5601
* **Elasticsearch :** http://localhost:9200

### √âtape 3 : Activer le Pipeline
1. Connectez-vous √† l'interface Airflow.
2. Localisez le DAG nomm√© `sky_safe_pipeline`.
3. Activez le bouton "Unpause" (le toggle switch) pour lancer l'ex√©cution automatis√©e toutes les 2 minutes.

---

## üéõÔ∏è Manuel de Pilotage au Quotidien

La commande `docker-compose up -d --build` a construit l'image Docker, branch√© l'infrastructure et d√©marr√© tous les services. Voici les 4 situations que vous rencontrerez au quotidien.

### 1. Juste apr√®s le premier `up -d --build` ‚Äî La V√©rification

L'usine tourne en arri√®re-plan (c'est le r√¥le du `-d` pour *detached*).

1. Ouvrez votre navigateur.
2. Acc√©dez √† `http://localhost:8080` (Airflow) et `http://localhost:5601` (Kibana).
3. Si les pages s'affichent, l'infrastructure est op√©rationnelle.

### 2. Modification du code Python (`src/` ou `dags/`)

> **Aucune action Docker requise.**

Les dossiers `src/`, `dags/` et `data/` sont mont√©s en tant que **volumes Docker** ‚Äî les conteneurs lisent directement les fichiers de votre machine en temps r√©el.

1. Modifiez votre code dans l'√©diteur.
2. Sauvegardez (`Ctrl + S`).
3. Airflow d√©tectera automatiquement la modification sous ~30 secondes et utilisera le nouveau code √† la prochaine ex√©cution.

### 3. Modification du `requirements.txt` ‚Äî Ajout d'une librairie

> **L'image doit √™tre reconstruite.**

Une nouvelle d√©pendance implique de rebuilder l'image Airflow. Ex√©cutez √† nouveau :

```bash
docker-compose up -d --build
```

Docker est incr√©mental : il ne recr√©e que les conteneurs impact√©s (Airflow), sans toucher √† Elasticsearch, Kibana ou la base de donn√©es.

### 4. Fin de journ√©e / Reprise le lendemain

**Le soir ‚Äî √âteindre l'infrastructure :**
```bash
docker-compose stop
```
Les conteneurs s'arr√™tent proprement. Les donn√©es dans `./data` et les configurations Airflow sont conserv√©es.

**Le lendemain ‚Äî Rallumer l'infrastructure :**
```bash
docker-compose start
```
L'infrastructure repart exactement dans l'√©tat o√π elle a √©t√© arr√™t√©e. Pas besoin de rebuild.

### 5. Suppression compl√®te des conteneurs et volumes

> **‚ö†Ô∏è Op√©ration destructive** ‚Äî toutes les donn√©es stock√©es dans les volumes (base de donn√©es Airflow, index Elasticsearch) seront effac√©es.

```bash
docker-compose down -v
```

√Ä utiliser uniquement pour repartir de z√©ro (reset complet) ou nettoyer l'environnement en fin de projet.

---

> **R√©sum√© :** Au quotidien ‚Üí `start` pour allumer, `stop` pour √©teindre, code normalement sur votre machine. Ne relancez `--build` que lors de l'ajout d'une nouvelle librairie dans `requirements.txt`. Utilisez `down -v` uniquement pour un reset complet.

---

## üë• L'√âquipe

Projet r√©alis√© dans le cadre du cours **DATA705 - BDD NoSQL** √† [T√©l√©com Paris](https://www.telecom-paris.fr/).

| Nom | GitHub |
|-----|--------|
| Tahiana Hajanirina Andriambahoaka | [@tahianahajanirina](https://github.com/tahianahajanirina) |
| Mohammed Ammar | [@mohammed-ammar](https://github.com/mohammed-ammar) |
| Lounis | [@lounis](https://github.com/lounis) |