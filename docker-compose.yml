version: '3.8'

# 1. ON SÉPARE LES VARIABLES D'ENVIRONNEMENT COMMUNES
x-airflow-env: &airflow-env
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
  AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
  AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
  # Clé secrète partagée entre webserver et scheduler (élimine le JWT InvalidSignatureError)
  AIRFLOW__CORE__SECRET_KEY: '${AIRFLOW__CORE__SECRET_KEY}'
  AIRFLOW__CORE__FERNET_KEY: '${AIRFLOW__CORE__FERNET_KEY}'
  AIRFLOW__WEBSERVER__SECRET_KEY: '${AIRFLOW__WEBSERVER__SECRET_KEY}'
  AIRFLOW__WEBSERVER__WORKERS: '1'
  AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT: '300'
  AIRFLOW__WEBSERVER__WEB_SERVER_MASTER_TIMEOUT: '300'
  AIRFLOW__PROVIDERS__GOOGLE__HIDDEN: 'True'
  PYTHONWARNINGS: 'ignore::UserWarning,ignore::SyntaxWarning,ignore::DeprecationWarning'
  # Supprime le warning Hadoop natif de Spark
  SPARK_HOME_WARN: 'false'
  HADOOP_HOME: '/tmp'

# 2. ON DÉFINIT LA RECETTE DE BASE AIRFLOW
x-airflow-common: &airflow-common
  build: .
  environment: *airflow-env
  volumes:
    - ./dags:/opt/airflow/dags
    - ./src:/opt/airflow/src
    - ./data:/opt/airflow/data
  depends_on:
    postgres:
      condition: service_healthy

services:
  # LE CERVEAU D'AIRFLOW
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  # INIT CONTAINER — Migre la BDD et crée l'utilisateur admin
  # (s'exécute une seule fois, les autres services attendent sa complétion)
  airflow-init:
    <<: *airflow-common
    environment: *airflow-env
    command: >
      bash -c "airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@skysafe.local"

  # L'INTERFACE WEB AIRFLOW
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8090:8080"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8080/health')\""]
      interval: 30s
      timeout: 15s
      retries: 10
      start_period: 90s

  # LE PLANIFICATEUR AIRFLOW
  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-webserver:
        condition: service_healthy

  # LA BASE DE DONNÉES FINALE
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.2
    environment:
      discovery.type: single-node
      xpack.security.enabled: 'false'
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
    ports:
      - "9200:9200"

  # LE DASHBOARD
  kibana:
    image: docker.elastic.co/kibana/kibana:8.10.2
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    depends_on:
      - elasticsearch